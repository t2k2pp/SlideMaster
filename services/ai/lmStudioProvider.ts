// =================================================================
// LM Studio Provider - ローカルLLM統合プロバイダー
// 2025年対応 (Llama 3.2, Mixtral, CodeLlama 対応)
// =================================================================

import { 
  AIProvider,
  AIProviderType,
  AIProviderConfig,
  TextGenerationRequest,
  ImageGenerationRequest,
  VideoAnalysisRequest,
  AIProviderError,
  AIProviderConnectionError,
  AIProviderConfigError,
  AIProviderRateLimitError
} from './aiProviderInterface';

import { getAvailableModels } from './modelRegistry';

// LM Studio API インターフェース (OpenAI互換)
interface LMStudioClient {
  chat: {
    completions: {
      create: (params: any) => Promise<any>;
    };
  };
  models: {
    list: () => Promise<any>;
  };
}

export class LMStudioProvider implements AIProvider {
  name: AIProviderType = 'lmstudio';
  private client: LMStudioClient;
  private config: AIProviderConfig;
  private baseURL: string;

  constructor(config: AIProviderConfig) {
    this.config = config;
    this.baseURL = config.endpoint || 'http://localhost:1234';

    try {
      this.client = this.createLMStudioClient(config);
    } catch (error) {
      throw new AIProviderConnectionError(
        `Failed to initialize LM Studio client: ${error.message}`,
        'lmstudio',
        error as Error
      );
    }
  }

  async generateText(request: TextGenerationRequest): Promise<string> {
    try {
      // LM Studioで利用可能なモデルを取得
      const availableModels = await this.getLoadedModels();
      const modelId = request.model && availableModels.includes(request.model) 
        ? request.model 
        : availableModels[0] || 'llama-3.2';
      
      const response = await this.client.chat.completions.create({
        model: modelId,
        messages: [
          ...(request.systemPrompt ? [{ role: 'system', content: request.systemPrompt }] : []),
          { role: 'user', content: request.prompt }
        ],
        temperature: request.temperature || 0.7,
        max_tokens: request.maxTokens || 2048,
        stream: false,
      });

      if (!response.choices || response.choices.length === 0) {
        throw new AIProviderError(
          'No response generated by LM Studio',
          'lmstudio'
        );
      }

      return response.choices[0].message.content || '';
    } catch (error) {
      throw this.handleLMStudioError(error, 'Text Generation');
    }
  }

  async generateImage(request: ImageGenerationRequest): Promise<string> {
    // LM Studioは通常テキスト生成専用
    throw new AIProviderError(
      'LM Studio does not support native image generation. Please use Fooocus for local image generation.',
      'lmstudio',
      'UNSUPPORTED_OPERATION'
    );
  }

  async analyzeVideo(request: VideoAnalysisRequest): Promise<string> {
    try {
      // マルチモーダル対応モデル（LLaVA等）の場合
      const availableModels = await this.getLoadedModels();
      const visionModel = availableModels.find(model => 
        model.toLowerCase().includes('llava') || 
        model.toLowerCase().includes('vision') ||
        model.toLowerCase().includes('multimodal')
      );

      if (!visionModel) {
        throw new AIProviderError(
          'No vision-capable model loaded in LM Studio. Please load a multimodal model (e.g., LLaVA).',
          'lmstudio',
          'MODEL_NOT_AVAILABLE'
        );
      }

      const response = await this.client.chat.completions.create({
        model: visionModel,
        messages: [{
          role: 'user',
          content: [
            { type: 'text', text: request.prompt },
            { 
              type: 'image_url', 
              image_url: { 
                url: request.videoData 
              } 
            }
          ]
        }],
        temperature: 0.3,
        max_tokens: 2048,
      });

      if (!response.choices || response.choices.length === 0) {
        throw new AIProviderError(
          'No analysis generated by LM Studio',
          'lmstudio'
        );
      }

      return response.choices[0].message.content || '';
    } catch (error) {
      throw this.handleLMStudioError(error, 'Video Analysis');
    }
  }

  async validateConfig(config: AIProviderConfig): Promise<boolean> {
    try {
      const endpoint = config.endpoint || 'http://localhost:1234';
      
      // LM Studioサーバーの生存確認
      const response = await fetch(`${endpoint}/v1/models`, {
        method: 'GET',
        headers: {
          'Content-Type': 'application/json'
        }
      });

      if (!response.ok) {
        return false;
      }

      const data = await response.json();
      return data.data && Array.isArray(data.data);
    } catch (error) {
      return false;
    }
  }

  async getAvailableModels(): Promise<{ [key: string]: string[] }> {
    try {
      const loadedModels = await this.getLoadedModels();
      
      // モデルをタイプ別に分類
      const textModels = loadedModels.filter(model => 
        !model.toLowerCase().includes('llava') && 
        !model.toLowerCase().includes('vision')
      );
      
      const visionModels = loadedModels.filter(model => 
        model.toLowerCase().includes('llava') || 
        model.toLowerCase().includes('vision') ||
        model.toLowerCase().includes('multimodal')
      );

      return {
        textGeneration: textModels,
        imageGeneration: [], // LM Studioは画像生成未対応
        videoAnalysis: visionModels,
      };
    } catch (error) {
      // フォールバック: レジストリからの固定リスト
      const textModels = getAvailableModels('lmstudio', 'text').map(m => m.id);
      const videoModels = getAvailableModels('lmstudio', 'video').map(m => m.id);

      return {
        textGeneration: textModels,
        imageGeneration: [],
        videoAnalysis: videoModels,
      };
    }
  }

  async estimateCost(request: any): Promise<number> {
    // ローカル実行のため実質無料
    return 0.0;
  }

  // LM Studio クライアント作成
  private createLMStudioClient(config: AIProviderConfig): LMStudioClient {
    const baseURL = config.endpoint || 'http://localhost:1234';
    
    return {
      chat: {
        completions: {
          create: async (params: any) => {
            const response = await fetch(`${baseURL}/v1/chat/completions`, {
              method: 'POST',
              headers: {
                'Content-Type': 'application/json',
                // LM StudioはAPI Key不要だが、互換性のため空文字を送信
                'Authorization': 'Bearer lm-studio'
              },
              body: JSON.stringify(params)
            });

            if (!response.ok) {
              const errorData = await response.json().catch(() => ({}));
              throw new Error(`LM Studio API error: ${response.status} ${response.statusText} - ${errorData.error?.message || 'Unknown error'}`);
            }

            return await response.json();
          }
        }
      },
      models: {
        list: async () => {
          const response = await fetch(`${baseURL}/v1/models`, {
            headers: {
              'Content-Type': 'application/json'
            }
          });

          if (!response.ok) {
            throw new Error(`LM Studio Models API error: ${response.status} ${response.statusText}`);
          }

          return await response.json();
        }
      }
    };
  }

  // 読み込み済みモデル取得
  private async getLoadedModels(): Promise<string[]> {
    try {
      const response = await this.client.models.list();
      return response.data.map((model: any) => model.id);
    } catch (error) {
      console.warn('Failed to get loaded models from LM Studio:', error);
      return ['llama-3.2']; // デフォルト
    }
  }

  // LM Studio固有エラーハンドリング
  private handleLMStudioError(error: unknown, context: string): AIProviderError {
    if (error instanceof AIProviderError) {
      return error;
    }

    if (error instanceof Error) {
      const message = error.message.toLowerCase();
      
      if (message.includes('connection') || message.includes('econnrefused')) {
        return new AIProviderConnectionError(
          'Cannot connect to LM Studio. Please ensure LM Studio is running on the specified port.',
          'lmstudio',
          error
        );
      }
      
      if (message.includes('model') && message.includes('not found')) {
        return new AIProviderConfigError(
          'No model loaded in LM Studio. Please load a model before making requests.',
          'lmstudio',
          error
        );
      }
      
      if (message.includes('timeout')) {
        return new AIProviderRateLimitError(
          'LM Studio request timeout. The model may be too large or busy.',
          'lmstudio',
          error
        );
      }
      
      return new AIProviderError(
        `LM Studio ${context} error: ${error.message}`,
        'lmstudio',
        'UNKNOWN_ERROR',
        error
      );
    }

    return new AIProviderError(
      `LM Studio ${context}: Unknown error occurred`,
      'lmstudio',
      'UNKNOWN_ERROR'
    );
  }

  // LM Studio特有のメソッド: サーバー情報取得
  async getServerInfo(): Promise<{
    version?: string;
    loadedModel?: string;
    modelPath?: string;
    contextLength?: number;
  }> {
    try {
      const modelsResponse = await this.client.models.list();
      const models = modelsResponse.data;
      
      if (models.length === 0) {
        return { loadedModel: 'None' };
      }

      const currentModel = models[0];
      return {
        loadedModel: currentModel.id,
        contextLength: currentModel.context_length || 4096
      };
    } catch (error) {
      console.warn('Failed to get LM Studio server info:', error);
      return {};
    }
  }
}